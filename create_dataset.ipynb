{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>candidato</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1442114017238028298</td>\n",
       "      <td>2021-11-03 00:00:02</td>\n",
       "      <td>@joseantoniokast @DanyJaneLpez1 Aqu√≠ lo que fr...</td>\n",
       "      <td>Kast</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1442114017238028298</td>\n",
       "      <td>2021-11-03 00:03:01</td>\n",
       "      <td>@joseantoniokast @DanyJaneLpez1 Tenemos que un...</td>\n",
       "      <td>Kast</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1442114017238028298</td>\n",
       "      <td>2021-11-03 00:06:37</td>\n",
       "      <td>@joseantoniokast @DanyJaneLpez1 Somos libres s...</td>\n",
       "      <td>Kast</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id                 date  \\\n",
       "0  1442114017238028298  2021-11-03 00:00:02   \n",
       "1  1442114017238028298  2021-11-03 00:03:01   \n",
       "2  1442114017238028298  2021-11-03 00:06:37   \n",
       "\n",
       "                                                text candidato   source  \n",
       "0  @joseantoniokast @DanyJaneLpez1 Aqu√≠ lo que fr...      Kast  twitter  \n",
       "1  @joseantoniokast @DanyJaneLpez1 Tenemos que un...      Kast  twitter  \n",
       "2  @joseantoniokast @DanyJaneLpez1 Somos libres s...      Kast  twitter  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_tweets = pd.read_csv(\"data/tweets_elections_raw.csv\",\n",
    "                        sep = \";\", \n",
    "                        encoding = \"utf-8\",\n",
    "                        quotechar = \"\\\"\",\n",
    "                        dtype={'id': object, 'user_id': object})  \n",
    "\n",
    "\n",
    "# Cluster 1: Boric, CLuster 2: Kast\n",
    "df_tweets[\"source\"] = \"twitter\"\n",
    "\n",
    "# Only rows with language = \"es\"\n",
    "df_tweets = df_tweets[df_tweets[\"language\"] == \"es\"]\n",
    "\n",
    "df_tweets = df_tweets.rename(columns={'tweet': 'text', 'cluster': 'candidato'})\n",
    "df_tweets = df_tweets[[\"user_id\", \"date\", \"text\", \"candidato\", \"source\"]]\n",
    "df_tweets = df_tweets.reset_index(drop=True)\n",
    "\n",
    "# Replace candidato == 1 with \"Boric\" and candidato == 2 with \"Kast\" \n",
    "df_tweets[\"candidato\"] = df_tweets[\"candidato\"].replace(1, \"Boric\")\n",
    "df_tweets[\"candidato\"] = df_tweets[\"candidato\"].replace(2, \"Kast\")\n",
    "\n",
    "df_tweets.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>candidato</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56973794459@s.whatsapp.net</td>\n",
       "      <td>2021-11-03 21:34:46</td>\n",
       "      <td>*AtreviDos*: Resumen Diario ‚úåüèº\\n\\n1Ô∏è‚É£ Por nues...</td>\n",
       "      <td>Kast</td>\n",
       "      <td>whatsapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56973794459@s.whatsapp.net</td>\n",
       "      <td>2021-11-04 20:45:55</td>\n",
       "      <td>üî¥ *AHORA*\\n\\nExisten prejuicios y caricaturas ...</td>\n",
       "      <td>Kast</td>\n",
       "      <td>whatsapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56973794459@s.whatsapp.net</td>\n",
       "      <td>2021-11-04 21:45:32</td>\n",
       "      <td>*AtreviDos*: Resumen Diario ‚úåüèº\\n\\n1Ô∏è‚É£ *Atr√©vet...</td>\n",
       "      <td>Kast</td>\n",
       "      <td>whatsapp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      user_id                date  \\\n",
       "0  56973794459@s.whatsapp.net 2021-11-03 21:34:46   \n",
       "1  56973794459@s.whatsapp.net 2021-11-04 20:45:55   \n",
       "2  56973794459@s.whatsapp.net 2021-11-04 21:45:32   \n",
       "\n",
       "                                                text candidato    source  \n",
       "0  *AtreviDos*: Resumen Diario ‚úåüèº\\n\\n1Ô∏è‚É£ Por nues...      Kast  whatsapp  \n",
       "1  üî¥ *AHORA*\\n\\nExisten prejuicios y caricaturas ...      Kast  whatsapp  \n",
       "2  *AtreviDos*: Resumen Diario ‚úåüèº\\n\\n1Ô∏è‚É£ *Atr√©vet...      Kast  whatsapp  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wsp = pd.read_pickle(\"data/wsp_elections_raw.pkl\")\n",
    "\n",
    "df_wsp = df_wsp.rename(columns={'remote_resource': 'user_id', 'data': 'text', 'key_remote_jid': 'group_id'})\n",
    "df_wsp = df_wsp[[\"user_id\", \"date\", \"text\", \"candidato\", \"key_from_me\"]]\n",
    "\n",
    "# Remove rows with key_from_me = 1\n",
    "df_wsp = df_wsp[df_wsp.key_from_me == 0]\n",
    "\n",
    "df_wsp = df_wsp.dropna(subset=[\"text\"])\n",
    "# Remove rows with empty text or NaNs:\n",
    "df_wsp = df_wsp[df_wsp.text.notnull()]\n",
    "df_wsp = df_wsp[df_wsp.text != '']\n",
    "del df_wsp[\"key_from_me\"]\n",
    "df_wsp = df_wsp.reset_index(drop=True)\n",
    "df_wsp[\"source\"] = \"whatsapp\"\n",
    "\n",
    "df_wsp.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_tweets, df_wsp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlortiz/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from preprocess import SpanishPreprocess \n",
    "import swifter \n",
    "\n",
    "sp = SpanishPreprocess(lower=True, remove_url=True, remove_hashtags = True, convert_emoticons=False, convert_emojis=False, normalize_inclusive_language=False, reduce_spam=True,\n",
    "                        remove_vowels_accents = True, remove_punctuation=True, remove_unprintable=True, remove_numbers=True, remove_stopwords=False, stopwords_list=None, stem=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bae3ea93b748d69d4524bdf638784d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1190797 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"text\"] = df[\"text\"].swifter.apply(sp.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.text.notnull()]\n",
    "df = df[df.text != '']\n",
    "df = df[df[\"text\"].apply(lambda x: type(x) == str)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Export df to csv in data folder with name \"data_pp.csv\"\n",
    "df.to_csv(\"data/data_pp.csv\", \n",
    "            sep = \";\", \n",
    "            encoding = \"UTF-8\",\n",
    "            quotechar = \"\\\"\",\n",
    "            index = False,\n",
    "            quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying texts:   0%|          | 9/1096848 [00:01<43:19:23,  7.03it/s] /home/jlortiz/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/transformers/pipelines/base.py:1036: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Classifying texts:   0%|          | 2154/1096848 [00:29<4:08:19, 73.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jlortiz/spanish_nlp/create_dataset.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000006vscode-remote?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m cl_name \u001b[39min\u001b[39;00m classifiers\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000006vscode-remote?line=30'>31</a>\u001b[0m     df[cl_name] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000006vscode-remote?line=31'>32</a>\u001b[0m     df[cl_name] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mprogress_apply(\u001b[39mlambda\u001b[39;49;00m x: predict_label(x, classifiers[cl_name], file_log))\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/tqdm/std.py:814\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[39m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(df, df_function)(wrapper, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    815\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m     t\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1144\u001b[0m             values,\n\u001b[1;32m   1145\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1147\u001b[0m         )\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/tqdm/std.py:809\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    804\u001b[0m     \u001b[39m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     \u001b[39m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    806\u001b[0m     \u001b[39m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[39m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     t\u001b[39m.\u001b[39mupdate(n\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39mtotal \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m t\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[0;32m--> 809\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/jlortiz/spanish_nlp/create_dataset.ipynb Cell 7\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000006vscode-remote?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m cl_name \u001b[39min\u001b[39;00m classifiers\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000006vscode-remote?line=30'>31</a>\u001b[0m     df[cl_name] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000006vscode-remote?line=31'>32</a>\u001b[0m     df[cl_name] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mprogress_apply(\u001b[39mlambda\u001b[39;00m x: predict_label(x, classifiers[cl_name], file_log))\n",
      "\u001b[1;32m/home/jlortiz/spanish_nlp/create_dataset.ipynb Cell 7\u001b[0m in \u001b[0;36mpredict_label\u001b[0;34m(text, model, file_log)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000006vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_label\u001b[39m(text, model, file_log):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000006vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000006vscode-remote?line=10'>11</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mpredict(text)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000006vscode-remote?line=11'>12</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000006vscode-remote?line=12'>13</a>\u001b[0m         time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/spanish_nlp/classifiers.py:231\u001b[0m, in \u001b[0;36mSpanishClassifier.predict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(text) \u001b[39m==\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m--> 231\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_hf_(text)\n\u001b[1;32m    232\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_prediction\n\u001b[1;32m    233\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(text) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n",
      "File \u001b[0;32m~/spanish_nlp/classifiers.py:221\u001b[0m, in \u001b[0;36mSpanishClassifier._predict_hf_\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict_hf_\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m--> 221\u001b[0m     prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(text, top_k\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_labels)\n\u001b[1;32m    222\u001b[0m     d_prediction \u001b[39m=\u001b[39m {}\n\u001b[1;32m    223\u001b[0m     \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prediction:\n\u001b[1;32m    224\u001b[0m         \u001b[39m#p[\"label\"] = self.labels[p[\"label\"]]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:138\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    105\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    139\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/transformers/pipelines/base.py:1067\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1066\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/transformers/pipelines/base.py:1074\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1073\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1074\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1075\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1076\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/transformers/pipelines/base.py:983\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    981\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m    982\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 983\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m    984\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    985\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:165\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1206\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1206\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1207\u001b[0m     input_ids,\n\u001b[1;32m   1208\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1209\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1210\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1211\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1212\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1213\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1214\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1215\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1216\u001b[0m )\n\u001b[1;32m   1217\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1218\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:848\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    839\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    841\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    842\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    843\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    846\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    847\u001b[0m )\n\u001b[0;32m--> 848\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    849\u001b[0m     embedding_output,\n\u001b[1;32m    850\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    851\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    852\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    853\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    854\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    855\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    856\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    857\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    858\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    859\u001b[0m )\n\u001b[1;32m    860\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    861\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    515\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    516\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    517\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    525\u001b[0m         hidden_states,\n\u001b[1;32m    526\u001b[0m         attention_mask,\n\u001b[1;32m    527\u001b[0m         layer_head_mask,\n\u001b[1;32m    528\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    529\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    530\u001b[0m         past_key_value,\n\u001b[1;32m    531\u001b[0m         output_attentions,\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:409\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    398\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    399\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    406\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    407\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    410\u001b[0m         hidden_states,\n\u001b[1;32m    411\u001b[0m         attention_mask,\n\u001b[1;32m    412\u001b[0m         head_mask,\n\u001b[1;32m    413\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    414\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    415\u001b[0m     )\n\u001b[1;32m    416\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    418\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:336\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    327\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    328\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    334\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    335\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 336\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    337\u001b[0m         hidden_states,\n\u001b[1;32m    338\u001b[0m         attention_mask,\n\u001b[1;32m    339\u001b[0m         head_mask,\n\u001b[1;32m    340\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    341\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    342\u001b[0m         past_key_value,\n\u001b[1;32m    343\u001b[0m         output_attentions,\n\u001b[1;32m    344\u001b[0m     )\n\u001b[1;32m    345\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    346\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:200\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    191\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    192\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 200\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[1;32m    202\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from classifiers import SpanishClassifier\n",
    "from tqdm import tqdm\n",
    "import datetime \n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "tqdm.pandas(desc='Classifying texts')\n",
    "\n",
    "def predict_label(text, model, file_log):\n",
    "    try:\n",
    "        return model.predict(text)\n",
    "    except Exception as e:\n",
    "        time = datetime.now().strftime(\"%d-%Y-%m %H:%M:%S\")\n",
    "        # Write log\n",
    "        with open(file_log, \"a\") as f:\n",
    "            f.write(f\"{time}. Error: {e}\\n\")\n",
    "            f.write(f\"{time}. Text: {text}\\n\")\n",
    "            #time.sleep(1)\n",
    "            # Delete cache GPU    \n",
    "        return None\n",
    "\n",
    "file_log = \"data/classification_log.txt\"\n",
    "\n",
    "classifiers_names = [\"hate_speech\"]#, \"toxic_speech\", \"sentiment_analysis\", \"emotion_analysis\", \"irony_analysis\", \"sexist_analysis\", \"racism_analysis\"]\n",
    "classifiers = {}\n",
    "\n",
    "for n in classifiers_names:\n",
    "    classifiers[n] = SpanishClassifier(model_name=n, device=0)\n",
    "\n",
    "for cl_name in classifiers.keys():\n",
    "    df[cl_name] = None\n",
    "    df[cl_name] = df[\"text\"].progress_apply(lambda x: predict_label(x, classifiers[cl_name], file_log))\n",
    "\n",
    "\n",
    "    # batch = 100_000\n",
    "    # window = (df.shape[0] // batch)+1\n",
    "    # for i in tqdm(range(window)):\n",
    "    #     i_init = i * batch\n",
    "    #     i_final = i_init + batch\n",
    "    #     if i_final > df.shape[0]-1:\n",
    "    #         i_final = df.shape[0]-1\n",
    "    #     texts = df[\"text\"].values[i_init:i_final].tolist()\n",
    "    #     predictions = classifiers[cl_name].predict(texts)\n",
    "\n",
    "    #     # Save prediction in column cl_name of df, with index i_init:i_final\n",
    "    #     # Create a new dataframe with predictions (list) and index between i_init and i_final. \n",
    "    #     df_p = pd.DataFrame(predictions, index=range(i_init:i_final))#index=df.index[i_init:i_final])\n",
    "\n",
    "    #     #For each column in df_p add prefix \"prediction_\" to the column name.\n",
    "    #     df_p.columns = [f\"{cl_name}__{col}\" for col in df_p.columns]\n",
    "        \n",
    "    #     # Merge df_p to df\n",
    "    #     df = df.merge(df_p, how=\"left\", left_index=True, right_index=True)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jlortiz/spanish_nlp/create_dataset.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000015vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgate.dcc.uchile.cl/home/jlortiz/spanish_nlp/create_dataset.ipynb#ch0000015vscode-remote?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mempty_cache()\n",
      "File \u001b[0;32m~/anaconda3/envs/spanish_nlp/lib/python3.9/site-packages/torch/cuda/memory.py:121\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 121\u001b[0m     torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_emptyCache()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>candidato</th>\n",
       "      <th>source</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>hate_speech__hateful_x</th>\n",
       "      <th>hate_speech__aggressive_x</th>\n",
       "      <th>hate_speech__targeted_x</th>\n",
       "      <th>hate_speech__hateful_y</th>\n",
       "      <th>...</th>\n",
       "      <th>hate_speech__targeted_x</th>\n",
       "      <th>hate_speech__aggressive_y</th>\n",
       "      <th>hate_speech__hateful_y</th>\n",
       "      <th>hate_speech__targeted_y</th>\n",
       "      <th>hate_speech__hateful_x</th>\n",
       "      <th>hate_speech__aggressive_x</th>\n",
       "      <th>hate_speech__targeted_x</th>\n",
       "      <th>hate_speech__hateful_y</th>\n",
       "      <th>hate_speech__aggressive_y</th>\n",
       "      <th>hate_speech__targeted_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1442114017238028298</td>\n",
       "      <td>2021-11-03 00:00:02</td>\n",
       "      <td>aqui lo que frenara y terminara con estos acto...</td>\n",
       "      <td>Kast</td>\n",
       "      <td>twitter</td>\n",
       "      <td>None</td>\n",
       "      <td>0.185040</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.185040</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1442114017238028298</td>\n",
       "      <td>2021-11-03 00:03:01</td>\n",
       "      <td>tenemos que unirnos ahora mas que nunca cuidar...</td>\n",
       "      <td>Kast</td>\n",
       "      <td>twitter</td>\n",
       "      <td>None</td>\n",
       "      <td>0.111021</td>\n",
       "      <td>0.049120</td>\n",
       "      <td>0.006238</td>\n",
       "      <td>0.111021</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1442114017238028298</td>\n",
       "      <td>2021-11-03 00:06:37</td>\n",
       "      <td>somos libres somos chile un pais luchador que ...</td>\n",
       "      <td>Kast</td>\n",
       "      <td>twitter</td>\n",
       "      <td>None</td>\n",
       "      <td>0.307856</td>\n",
       "      <td>0.109812</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.307856</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1442114017238028298</td>\n",
       "      <td>2021-11-06 08:06:12</td>\n",
       "      <td>y le falto cafiches acomplejados frustrados etc</td>\n",
       "      <td>Kast</td>\n",
       "      <td>twitter</td>\n",
       "      <td>None</td>\n",
       "      <td>0.609422</td>\n",
       "      <td>0.277636</td>\n",
       "      <td>0.010943</td>\n",
       "      <td>0.609422</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1442114017238028298</td>\n",
       "      <td>2021-11-06 08:07:10</td>\n",
       "      <td>aaaaaah mi ni√±o igual ud que tengo un maravill...</td>\n",
       "      <td>Kast</td>\n",
       "      <td>twitter</td>\n",
       "      <td>None</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>0.019424</td>\n",
       "      <td>0.021199</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096843</th>\n",
       "      <td>56963532690@s.whatsapp.net</td>\n",
       "      <td>2021-12-20 02:46:24</td>\n",
       "      <td>que diga que es en el extranjero</td>\n",
       "      <td>Kast</td>\n",
       "      <td>whatsapp</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096844</th>\n",
       "      <td>56999884716@s.whatsapp.net</td>\n",
       "      <td>2021-12-20 02:50:37</td>\n",
       "      <td>se viene</td>\n",
       "      <td>Kast</td>\n",
       "      <td>whatsapp</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096845</th>\n",
       "      <td>56999884716@s.whatsapp.net</td>\n",
       "      <td>2021-12-20 02:50:42</td>\n",
       "      <td>una dictadura</td>\n",
       "      <td>Kast</td>\n",
       "      <td>whatsapp</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096846</th>\n",
       "      <td>56963532690@s.whatsapp.net</td>\n",
       "      <td>2021-12-20 02:51:08</td>\n",
       "      <td>tranqui</td>\n",
       "      <td>Kast</td>\n",
       "      <td>whatsapp</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096847</th>\n",
       "      <td>56963532690@s.whatsapp.net</td>\n",
       "      <td>2021-12-20 02:51:34</td>\n",
       "      <td>no hay que preocuparse por ahora</td>\n",
       "      <td>Kast</td>\n",
       "      <td>whatsapp</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096848 rows √ó 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id                 date  \\\n",
       "0               1442114017238028298  2021-11-03 00:00:02   \n",
       "1               1442114017238028298  2021-11-03 00:03:01   \n",
       "2               1442114017238028298  2021-11-03 00:06:37   \n",
       "3               1442114017238028298  2021-11-06 08:06:12   \n",
       "4               1442114017238028298  2021-11-06 08:07:10   \n",
       "...                             ...                  ...   \n",
       "1096843  56963532690@s.whatsapp.net  2021-12-20 02:46:24   \n",
       "1096844  56999884716@s.whatsapp.net  2021-12-20 02:50:37   \n",
       "1096845  56999884716@s.whatsapp.net  2021-12-20 02:50:42   \n",
       "1096846  56963532690@s.whatsapp.net  2021-12-20 02:51:08   \n",
       "1096847  56963532690@s.whatsapp.net  2021-12-20 02:51:34   \n",
       "\n",
       "                                                      text candidato  \\\n",
       "0        aqui lo que frenara y terminara con estos acto...      Kast   \n",
       "1        tenemos que unirnos ahora mas que nunca cuidar...      Kast   \n",
       "2        somos libres somos chile un pais luchador que ...      Kast   \n",
       "3          y le falto cafiches acomplejados frustrados etc      Kast   \n",
       "4        aaaaaah mi ni√±o igual ud que tengo un maravill...      Kast   \n",
       "...                                                    ...       ...   \n",
       "1096843                   que diga que es en el extranjero      Kast   \n",
       "1096844                                           se viene      Kast   \n",
       "1096845                                      una dictadura      Kast   \n",
       "1096846                                            tranqui      Kast   \n",
       "1096847                   no hay que preocuparse por ahora      Kast   \n",
       "\n",
       "           source hate_speech  hate_speech__hateful_x  \\\n",
       "0         twitter        None                0.185040   \n",
       "1         twitter        None                0.111021   \n",
       "2         twitter        None                0.307856   \n",
       "3         twitter        None                0.609422   \n",
       "4         twitter        None                0.020156   \n",
       "...           ...         ...                     ...   \n",
       "1096843  whatsapp        None                     NaN   \n",
       "1096844  whatsapp        None                     NaN   \n",
       "1096845  whatsapp        None                     NaN   \n",
       "1096846  whatsapp        None                     NaN   \n",
       "1096847  whatsapp        None                     NaN   \n",
       "\n",
       "         hate_speech__aggressive_x  hate_speech__targeted_x  \\\n",
       "0                         0.100800                 0.006035   \n",
       "1                         0.049120                 0.006238   \n",
       "2                         0.109812                 0.005927   \n",
       "3                         0.277636                 0.010943   \n",
       "4                         0.019424                 0.021199   \n",
       "...                            ...                      ...   \n",
       "1096843                        NaN                      NaN   \n",
       "1096844                        NaN                      NaN   \n",
       "1096845                        NaN                      NaN   \n",
       "1096846                        NaN                      NaN   \n",
       "1096847                        NaN                      NaN   \n",
       "\n",
       "         hate_speech__hateful_y  ...  hate_speech__targeted_x  \\\n",
       "0                      0.185040  ...                      NaN   \n",
       "1                      0.111021  ...                      NaN   \n",
       "2                      0.307856  ...                      NaN   \n",
       "3                      0.609422  ...                      NaN   \n",
       "4                      0.020156  ...                      NaN   \n",
       "...                         ...  ...                      ...   \n",
       "1096843                     NaN  ...                      NaN   \n",
       "1096844                     NaN  ...                      NaN   \n",
       "1096845                     NaN  ...                      NaN   \n",
       "1096846                     NaN  ...                      NaN   \n",
       "1096847                     NaN  ...                      NaN   \n",
       "\n",
       "         hate_speech__aggressive_y  hate_speech__hateful_y  \\\n",
       "0                              NaN                     NaN   \n",
       "1                              NaN                     NaN   \n",
       "2                              NaN                     NaN   \n",
       "3                              NaN                     NaN   \n",
       "4                              NaN                     NaN   \n",
       "...                            ...                     ...   \n",
       "1096843                        NaN                     NaN   \n",
       "1096844                        NaN                     NaN   \n",
       "1096845                        NaN                     NaN   \n",
       "1096846                        NaN                     NaN   \n",
       "1096847                        NaN                     NaN   \n",
       "\n",
       "         hate_speech__targeted_y  hate_speech__hateful_x  \\\n",
       "0                            NaN                     NaN   \n",
       "1                            NaN                     NaN   \n",
       "2                            NaN                     NaN   \n",
       "3                            NaN                     NaN   \n",
       "4                            NaN                     NaN   \n",
       "...                          ...                     ...   \n",
       "1096843                      NaN                     NaN   \n",
       "1096844                      NaN                     NaN   \n",
       "1096845                      NaN                     NaN   \n",
       "1096846                      NaN                     NaN   \n",
       "1096847                      NaN                     NaN   \n",
       "\n",
       "         hate_speech__aggressive_x  hate_speech__targeted_x  \\\n",
       "0                              NaN                      NaN   \n",
       "1                              NaN                      NaN   \n",
       "2                              NaN                      NaN   \n",
       "3                              NaN                      NaN   \n",
       "4                              NaN                      NaN   \n",
       "...                            ...                      ...   \n",
       "1096843                        NaN                      NaN   \n",
       "1096844                        NaN                      NaN   \n",
       "1096845                        NaN                      NaN   \n",
       "1096846                        NaN                      NaN   \n",
       "1096847                        NaN                      NaN   \n",
       "\n",
       "         hate_speech__hateful_y  hate_speech__aggressive_y  \\\n",
       "0                           NaN                        NaN   \n",
       "1                           NaN                        NaN   \n",
       "2                           NaN                        NaN   \n",
       "3                           NaN                        NaN   \n",
       "4                           NaN                        NaN   \n",
       "...                         ...                        ...   \n",
       "1096843                     NaN                        NaN   \n",
       "1096844                     NaN                        NaN   \n",
       "1096845                     NaN                        NaN   \n",
       "1096846                     NaN                        NaN   \n",
       "1096847                     NaN                        NaN   \n",
       "\n",
       "         hate_speech__targeted_y  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  \n",
       "...                          ...  \n",
       "1096843                      NaN  \n",
       "1096844                      NaN  \n",
       "1096845                      NaN  \n",
       "1096846                      NaN  \n",
       "1096847                      NaN  \n",
       "\n",
       "[1096848 rows x 42 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spanish_nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a17d0dd0e62b0b2f91548d65570879f0f98cd396de4905819b9ef013dc03a942"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
